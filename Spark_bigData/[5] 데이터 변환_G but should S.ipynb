{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ETL (Extract, Transform, Load)**은 소스에서 필요한 데이터를 추출, 변환하고, 다른 타켓으로 로딩하는 것으로 말한다.\n",
    "\n",
    "- **추출**은 원천에서 데이터를 가져오는 것으로, spark에서는 예를 들어 csv, NoSQL 등에서 데이터를 읽어서 RDD, DataFrame을 생성하는 작업을 말한다.\n",
    "- **변환** 은 분석 가능한 형식으로 변환하는 것으로, 결측 값이나 이상값을 제거하거나 map() 함수를 사용하거나 데이터타입 변환을 하는 등의 작업을 한다.\n",
    "- **로딩**은 변환한 데이터를 저장하여 놓는 것으로 Spark에서는 RDD, DataFrame의 형식으로 만들어 놓는다. 지도학습 Supervised Learning을 하려면, DataFrame은 label, features 컬럼을, **RDD는 label과 features를 가지고 있는 Labeled Point로 구성** 해야 한다.\n",
    "\n",
    "-------------------------------------------------\n",
    "### [Source] -extract-> [transfrom] -load-> [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RDD에서만 읽고 데이터를 변환하고 mlib라이브러리를 사용해서 함\n",
    "## 데이터 프레임이 나오고 나오서는 ml 라이브러리가 추가가되어서 사용되고 있음\n",
    "## mlib는 유지보수로 지원됨. 즉, ml라이브러리를 사용하는 것이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패키지 | 설명 | 데이터 타잎 예\n",
    "-------|-------|-----\n",
    "```mllib``` | RDD API | ```pyspark.mllib.linalg.Vector```, ```pyspark.mllib.linalg.Matrix```\n",
    "```ml``` | DataFrame API | ```pyspark.ml.linalg.Vector```, ```pyspark.ml.linalg.Matrix```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RDD 변환\n",
    "\n",
    "기계학습을 하기 위해서는 데이터를 일정 형식으로 만들어 주어야 한다.\n",
    "**```Vector```**, **```Labeled Point```**, **```Matrix```**를 배워보자.\n",
    "\n",
    "구분 | 설명\n",
    "----------|----------\n",
    "```Vector``` | ```numpy vector```와 같은 기능을 한다. **dense**와 **sparse** vector로 구분한다.\n",
    "```Labeled Point``` | 분류를 의미하는 클래스 또는 **label**과 속성 **features** 이 묶인 구조로서, 지도학습 supervised learning을 할 경우 사용된다.\n",
    "```Matrix``` | ```numpy matrix```와 같은 특징을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 RDD - vectors\n",
    "\n",
    "행렬 **Vector**는 **dense**와 **sparse**로 구분할 수 있다.\n",
    "\n",
    "* dense vector는 빈 값이 별로 없이 **모든** 행열이 값을 가지고 있다.\n",
    "* sparse vector는 빈 값이 많아서, 값이 있는 경우 그 값이 있는 **인덱스**로 표현해 배열을 축약하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Vectors 밀집벡터\n",
    "\n",
    "벡터는 일련의 수로 구성이 되고, 행벡터 또는 열벡터가 될 수 있다. 채워지는 값이 대부분 0이면 희소벡터 Sparse Vectors로 만들어 질 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이 array 를 사용하여 만든 dense vector\n",
    "import numpy as np\n",
    "dv = np.array([1.0, 2.1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector: [1.0,2.1,3.0]\n",
      " Type: <class 'pyspark.mllib.linalg.DenseVector'>\n"
     ]
    }
   ],
   "source": [
    "# Spark 를 사용하여 만든 dense vector\n",
    "# Spark에서는 vector 명령을 통해 벡터 만들기 가능 (RDD: mlib모듈, DataFrame: ml모듈 사용)\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "dv1mllib=Vectors.dense([1.0, 2.1, 3])\n",
    "\n",
    "print (\"Dense vector: {}\\n Type: {}\".format(dv1mllib, type(dv1mllib)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml의 dense vector: [1.0,2.1,3.0]\n"
     ]
    }
   ],
   "source": [
    "# pyspark.ml 모듈을 사용하여 Vectors 제작\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dv1ml=Vectors.dense([1.0, 2.1, 3])\n",
    "print (\"ml의 dense vector: {}\".format(dv1ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.1 3.0 "
     ]
    }
   ],
   "source": [
    "#dense vectors는 numpy array와 같은 특징, \n",
    "#인덱스로 값을 읽을 수 있음, 반복문에서 사용할 수 있다.\n",
    "for e in dv1mllib:\n",
    "    print (e, end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 벡터와 같이 **product**, **dot**, **norm**과 같은 벡터 연산을 할 수도 있다.\n",
    "결과 값은 numpy와 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.41"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv1mllib.dot(dv1mllib) #내적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.41"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(dv,dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 4.41, 9.0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#더하기,빼기, 곱하기, 나누기 연산도 가능 dot와 달리 항목별로 실행됨\n",
    "dv1mllib*dv1mllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Vectors 희소행렬\n",
    "###### 0이 있는 것을 지우는 것이 희소행렬임\n",
    "\n",
    "행렬에는 0 값이 많이 존재하기 때문에, 0값이 아닌 **NZ Nonzero**만 저장하면 훨씬 효율적이다.\n",
    "**sparse**는 실제 **값이 없는 요소, '0'을 제거**하여 만든 vector이다.\n",
    "Spark에서 type field (1 바이트 길이)를 통해 식별한다 (0: sparse, 1: dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어, 다음은 1차원 dense vector이다.\n",
    "```python\n",
    "[160, 69, 0, 0, 24]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv1 = Vectors.sparse(5,[0,1,4],[160.0,69.0,24.0]) #요소가 다섯개, 0번째, 1번째, 4번째만 값이 있는데, 그것이 뒤에 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.linalg.SparseVector"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sv1) #ml임을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([160.,  69.,   0.,   0.,  24.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Array을 사용하여 sparse에서 dense 벡터로 바꾸기\n",
    "\n",
    "sv1.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Matrix\n",
    "\n",
    "로컬 Matrix 역시 밀집 dense, 희소 sparse형식을 지원한다.\n",
    "\n",
    "mlib 사용\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.linalg import Matrices \n",
    "Matrices.dense(3, 2, [1,2,3,4,5,6])\n",
    "```\n",
    "\n",
    "넘파이로 행, 열, 값 나눈후 scipy.sparse 사용\n",
    "```python\n",
    "import numpy as np\n",
    "np.array([]) #numpy로 행,열,값 분류 후 scipy사용하여 sparse matrix 만들기\n",
    "\n",
    "import scipy.sparse as sps\n",
    "mtx = sps.csc_matrix((data, (row, col)), shape=(3, 3))\n",
    "mx.todense() #Dense로 변환\n",
    "```\n",
    "<br>\n",
    "\n",
    "이거 사용하면 **sparse Matrix** 만들기 가능\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.linalg import Matrices \n",
    "Matrices.dense()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "Matrices.dense(3, 2, [1,2,3,4,5,6]) # 3*2 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Vectors\n",
    "\n",
    "```python\n",
    "[1 0 2]\n",
    "[0 0 3]\n",
    "[4 5 6]\n",
    "```\n",
    "위와 같은 2차원 dense vectors를 sparse vectors의 배열 방식으로 표현해보자.\n",
    "우선 다음과 같이\n",
    "행, 열, 값 vector를 만든다.\n",
    "\n",
    "```python\n",
    "행 | 0 | 0 | 1 | 2 | 2 | 2\n",
    "열 | 0 | 2 | 2 | 0 | 1 | 2\n",
    "값 | 1 | 2 | 3 | 4 | 5 | 6\n",
    "```\n",
    "**행, 열, 데이터를 한 쌍**으로 읽으면 된다.\n",
    "즉 행 0, 열 0의 위치에 1, 행 0, 열 2의 위치에 2. 이런 식으로 6개의 데이터가 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps\n",
    "\n",
    "mtx = sps.csc_matrix((data, (row, col)), shape=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 2]\n",
      " [0 0 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "print (mtx.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Vectors의 CSR (Compressed Sparse Row) 또는 Yale Format\n",
    "#### Sparse Vectors 변환시켜주기\n",
    "\n",
    "다음 5개의 값으로 표현된다.\n",
    "* 첫째 행 개수 (```int```)\n",
    "* 둘째 열 개수 (```int```)\n",
    "* 세째 ```int[]```은 **열 포인터** (IA):\n",
    "    * IA[0]=**0** 시작 값은 0으로, IA[i]=IA[i-1] + (i-1)열의 **NNZ** (NZ 개수, Num of NonZeroes)\n",
    "* 네째 ```int[]```은 **행 인덱스** (JA): 각 NZ의 행 인덱스\n",
    "* 마지막은 소수 (```double```)로 실제 값 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "# 6*4 2차원 배열\n",
    "dm = Matrices.dense(6,4,[1, 2, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 5, 6, 7, 0, 0, 0, 0, 0, 0, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [2., 3., 0., 0.],\n",
       "       [0., 0., 5., 0.],\n",
       "       [0., 4., 6., 0.],\n",
       "       [0., 0., 7., 0.],\n",
       "       [0., 0., 0., 8.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMatrix(6, 4, [0, 2, 4, 7, 8], [0, 1, 1, 3, 2, 3, 4, 5], [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#희소행렬로 변환한 값\n",
    "dm.toSparse() \n",
    "# 행의 갯수: 6\n",
    "# 열의 갯수: 4\n",
    "# 열로 볼때 nonzero값 갯수 [2,2,3,1]  -> 하나씩 더한 값 [0,2,4,7]\n",
    "# 행으로 볼때 nonzero 값의 갯수 [0,1,1,3,2,3,4,5] 행을 기준으로 따라가야됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[9., 0.],\n",
      "             [0., 8.],\n",
      "             [0., 6.]])\n"
     ]
    }
   ],
   "source": [
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])\n",
    "d=sm.toDense()\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 분산 Matrix\n",
    "\n",
    "partition으로 나눠서 Matrix가 존재함.\n",
    "- spark는 분산에서 사용, pandas는 아님\n",
    "\n",
    "배열은 n차원을 가질 수 있고, 2차원인 경우에는 매트릭스라고 지칭한다.\n",
    "매트릭스 역시 **로컬과 분산**으로 구분할 수 있다. <br>\n",
    "<br>\n",
    "#### mlib 라이브러리 내에 존재\n",
    "**로컬 매트릭스**로 ```pyspark.mllib.linalg.Matrix, Matrices```를 사용한다.<br> \n",
    "**분산 매트릭스**는 당연히 여러 노드에 분산해서 사용할 수 있고, ```pyspark.mllib.linalg.distributed```에 존재하는 Row Matrix, Indexed Row Matrix, Coordinate Matrix, Block Matrix를 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row Matrix\n",
    "\n",
    "RowMatrix는 ```pyspark.mllib.linalg.distributed```에서 제공되는 분산벡터. <br>\n",
    "**RDD vectors**로부터 생성된다.<br>\n",
    "우선 리스트에서 RDD를 생성하고 이를 RowMatrix에 넘겨주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [1.1, 2.1, 3.1], [1.2, 2.2, 3.3]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [[1.0,2.0,3.0],[1.1,2.1,3.1],[1.2,2.2,3.3]]\n",
    "my=spark.sparkContext.parallelize(p)\n",
    "my.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD Vectors를 넘겨주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.linalg.distributed.RowMatrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.0, 2.0, 3.0]),\n",
       " DenseVector([1.1, 2.1, 3.1]),\n",
       " DenseVector([1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "rm=RowMatrix(my)\n",
    "print (type(rm))\n",
    "rm.rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.numRows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.numCols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexed Row Matrix\n",
    "\n",
    "RDD로 만들어야됨 <br>\n",
    "IndexedRow(데이터순서,[]) <br>\n",
    "앞서 Row Matrix과 유사하지만, 파티션으로 나누어, 그러나 순서를 지켜서 저장이 된다.\n",
    "따라서 시계열 데이터와 같이 순서가 있는 데이터를 저장하기에 적합하다.\n",
    "\n",
    "순서인덱스와 벡터로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow\n",
    "\n",
    "irRdd = spark.sparkContext.parallelize([\n",
    "    IndexedRow(1, [3, 1, 2]),\n",
    "    IndexedRow(2, [1, 3, 2]),\n",
    "    IndexedRow(3, [5, 4, 3]),\n",
    "    IndexedRow(4, [6, 7, 4]),\n",
    "    IndexedRow(5, [8, 9, 2]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IndexedRow(1, [3.0,1.0,2.0]),\n",
       " IndexedRow(2, [1.0,3.0,2.0]),\n",
       " IndexedRow(3, [5.0,4.0,3.0])]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irRdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
    "\n",
    "irm = IndexedRowMatrix(irRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(irm.numRows()) #0부터 시작한다치고\n",
    "print(irm.numCols()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Labeled Point\n",
    "\n",
    "- Labeled point는 로컬벡터로 레이블을 가지고 있는 밀집 또는 희소 행렬을 말한다.\n",
    "- 레이블이 있으므로, supervised learning에 요구되는 형식이다.\n",
    "- 레이블은 double형식으로 저장되어야 한다.\n",
    "\n",
    "#### label, features로 구성\n",
    "\n",
    "**분류** 및 **회귀분석**에 사용되는 데이터 타잎이다.\n",
    "**'label'**과 **'features'**로 구성된다.\n",
    "\n",
    "구분 | 지도학습을 하기 위한 label과 features의 구성\n",
    "-----|-----\n",
    "label | supervised learning에서 '구분 값'으로 사용한다. 데이터타입은 'DoubleType'으로 설정되어야 한다.\n",
    "features | **sparse**, **dense** 모두 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.0,3.0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label 형태\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "LabeledPoint(1.0, [1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1992.0, (10,[0,1,2],[3.0,5.5,10.0]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sparse vectors로 feature 구성\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "LabeledPoint(1992, Vectors.sparse(10, {0: 3.0, 1:5.5, 2: 10.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서로 다른 패키지의 데이터타잎 **```mllib LabeledPoint```**와 **```ml Vectors```**를 혼용하면, 형변환 오류가 발생한다.\n",
    "이러한 오류는 패키지를 혼용하지 않으면 된다.\n",
    "\n",
    "```python\n",
    "Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "```\n",
    "\n",
    "**```dv1mllib```**은 앞서 **```mllib```**로부터 생성된 dense vector이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All from mlib\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "LabeledPoint(1.0, dv1mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fromML\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "LabeledPoint(1.0, Vectors.fromML(dv1ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD 데이터를 LabeledPoint 로 변환하기\n",
    "spark에서 제공한 **데이터파일 data/mlib/sample_svm_data.txt**을 읽어서 훈련데이터를 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "_fsvm = os.path.join(\"sample_svm_data.txt\")\n",
    "\n",
    "#하나씩 읽어보기\n",
    "try:\n",
    "    _f=open(_fsvm,'r')\n",
    "    _lines=_f.readlines()\n",
    "    _f.close()\n",
    "except:\n",
    "    print(\"An exception occurred\")\n",
    "    \n",
    "#파일로부터 데이터를 readlines() 함수로 모두 읽어온다.\n",
    "#첫행에 label, features 로 구성되어있음\n",
    "_lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark에서 RDD 생성\n",
    "\n",
    "Spark는 파일을 Python을 통하지 않고, 직접 읽을 수 있다. <br>\n",
    "원본 데이터 ```sample_svm_data.txt```는 공백으로 구분되어 있다.<br>\n",
    "읽을 대상이 파일이므로, RDD를 사용한다. 각 행을 공백으로 분리하여 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rdd=spark.sparkContext.textFile(_fsvm)\\\n",
    "    .map(lambda line: [float(x) for x in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.0,\n",
       " 2.52078447201548,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.004684436494304,\n",
       " 2.000347299268466,\n",
       " 0.0,\n",
       " 2.228387042742021,\n",
       " 2.228387042742023,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#첫째 행 읽기\n",
    "_rdd.take(2)[0]\n",
    "#_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabeledPoint 생성\n",
    "\n",
    "위 데이터에서 보듯이 첫 열은 **label**로, 그 나머지는 **features**로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "_trainRdd0=_rdd.map(lambda line:LabeledPoint(line[0], line[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447201548,0.0,0.0,0.0,2.004684436494304,2.000347299268466,0.0,2.228387042742021,2.228387042742023,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd0.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "공백을 분리하고, 분리된 데이터를 labeled point로 구성하는 기능을 합쳐서 실행해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainRdd=spark.sparkContext.textFile(_fsvm)\\\n",
    "    .map(lambda line: [float(x) for x in line.split()])\\\n",
    "    .map(lambda p:LabeledPoint(p[0], p[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447201548,0.0,0.0,0.0,2.004684436494304,2.000347299268466,0.0,2.228387042742021,2.228387042742023,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447201548,0.0,0.0,0.0,2.004684436494304,2.000347299268466,0.0,2.228387042742021,2.228387042742023,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###정리 // split 하고 label 분리\n",
    "def createLP(line):\n",
    "    p = [float(x) for x in line.split()]\n",
    "    return LabeledPoint(p[0], p[1:])\n",
    "\n",
    "\n",
    "_fp = os.path.join(\"sample_svm_data.txt\")\n",
    "\n",
    "_rdd=spark.sparkContext.textFile(_fp)\n",
    "trainRdd = _rdd.map(createLP)\n",
    "\n",
    "trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. svm의 입력파일 형식\n",
    "\n",
    "limsvm은 기계학습 모델인 svm을 위한 입력데이터 형식이다.\n",
    "0은 label, 나머지는 index:value 쌍으로 구성한다.<br>\n",
    "sparse들을 위함\n",
    "\n",
    "```python\n",
    "[label] [index1]:[value1] [index2]:[value2] ...\n",
    "[label] [index1]:[value1] [index2]:[value2] ...\n",
    "```\n",
    "\n",
    "* 예\n",
    "```python\n",
    "0 128:51 129:159 130:253 131:159 132:50 155:48 156:238 157:252 158:252 159:252 160:237 182:54 183:227 184:253 185:252 186:239 187:233 ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DataFrame 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-53-93634b8eb97a>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-53-93634b8eb97a>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    return LabeledPoint(p[0], p[1:]) <- 첫째는 label로, 나머지는 features\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "fsvm = os.path.join(\"sample_libsvm_data.txt\")\n",
    "#dfsvm = spark.read.format(\"libsvm\").load(fsvm)\n",
    "#하나씩 읽어보기\n",
    "\n",
    "dfsvm = spark.read.format('libsvm').load(fsvm)\n",
    "\n",
    "trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dfsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsvm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame에서는 label, features 컬럼이 별도로 생성되고, features는 sparse vectors 형식으로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='0 128:51 129:159 130:253 131:159 132:50 155:48 156:238 157:252 158:252 159:252 160:237 182:54 183:227 184:253 185:252 186:239 187:233 188:252 189:57 190:6 208:10 209:60 210:224 211:252 212:253 213:252 214:202 215:84 216:252 217:253 218:122 236:163 237:252 238:252 239:252 240:253 241:252 242:252 243:96 244:189 245:253 246:167 263:51 264:238 265:253 266:253 267:190 268:114 269:253 270:228 271:47 272:79 273:255 274:168 290:48 291:238 292:252 293:252 294:179 295:12 296:75 297:121 298:21 301:253 302:243 303:50 317:38 318:165 319:253 320:233 321:208 322:84 329:253 330:252 331:165 344:7 345:178 346:252 347:240 348:71 349:19 350:28 357:253 358:252 359:195 372:57 373:252 374:252 375:63 385:253 386:252 387:195 400:198 401:253 402:190 413:255 414:253 415:196 427:76 428:246 429:252 430:112 441:253 442:252 443:148 455:85 456:252 457:230 458:25 467:7 468:135 469:253 470:186 471:12 483:85 484:252 485:223 494:7 495:131 496:252 497:225 498:71 511:85 512:252 513:145 521:48 522:165 523:252 524:173 539:86 540:253 541:225 548:114 549:238 550:253 551:162 567:85 568:252 569:249 570:146 571:48 572:29 573:85 574:178 575:225 576:253 577:223 578:167 579:56 595:85 596:252 597:252 598:252 599:229 600:215 601:252 602:252 603:252 604:196 605:130 623:28 624:199 625:252 626:252 627:253 628:252 629:252 630:233 631:145 652:25 653:128 654:252 655:253 656:252 657:141 658:37')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsvm.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TF\n",
    "\n",
    "지금까지는 정량데이터를 다루었지만, **텍스트**를 변환해보자.\n",
    "##### TF (Term Frequency)\n",
    "단어빈도를 계산하기 위해 HashingTF를 사용할 수 있다.\n",
    "단어ID로 Hash 알고리즘에 따라 무작위 번호를 생성하고, 단어빈도를 생성한다.\n",
    "Hash를 사용하지 않고 계산한 단어빈도는 당연히 동일하다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiRdd3 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"s-master/data\",\"ds_spark_wiki.txt\"))\\\n",
    "    .map(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {1026674: 1.0}),\n",
       " SparseVector(1048576, {148618: 1.0, 183975: 1.0, 216207: 1.0, 261052: 1.0, 617454: 1.0, 696349: 1.0, 721336: 1.0, 816618: 1.0, 897662: 1.0}),\n",
       " SparseVector(1048576, {60386: 1.0, 177421: 1.0, 568609: 1.0, 569458: 1.0, 847171: 1.0, 850510: 1.0, 1040679: 1.0}),\n",
       " SparseVector(1048576, {261052: 4.0, 816618: 4.0}),\n",
       " SparseVector(1048576, {60386: 4.0, 594754: 4.0}),\n",
       " SparseVector(1048576, {21980: 1.0, 70882: 1.0, 274690: 1.0, 357784: 1.0, 549790: 1.0, 597434: 1.0, 804583: 1.0, 829803: 1.0, 935701: 1.0}),\n",
       " SparseVector(1048576, {154253: 1.0, 261052: 1.0, 438276: 1.0, 460085: 1.0, 585459: 1.0, 664288: 1.0, 816618: 1.0, 935701: 2.0, 948143: 1.0, 1017889: 1.0}),\n",
       " SparseVector(1048576, {270017: 1.0, 472985: 1.0, 511771: 1.0, 718483: 1.0, 820917: 1.0}),\n",
       " SparseVector(1048576, {34116: 1.0, 87407: 1.0, 276491: 1.0, 348943: 1.0, 482882: 1.0, 549350: 1.0, 721336: 1.0, 816618: 1.0, 1025622: 1.0}),\n",
       " SparseVector(1048576, {1769: 1.0, 151357: 1.0, 500659: 1.0, 547760: 1.0, 979482: 1.0})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(wikiRdd3)\n",
    "tf.collect()\n",
    "\n",
    "# (벡터데이터수, {해싱아이디(컬럼번호): 분포})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TF-IDF\n",
    "\n",
    "IDF는 전체에서 몇 개의 문서에 씌였는지를 반대로 계산한 값이다.\n",
    "뒤 DataFrame를 사용하여 TF-IDF를 계산하면서 자세히 설명하기로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF  #mlib니까 RDD임을 알 수 잇음\n",
    "\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "#SparseVector(1048576, {1026674: 1.7047 # 계산된 결과가 들어가서 다름 #빈도가 TF빈도가 아닌 TF-IDF임}),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {1026674: 1.7047}),\n",
       " SparseVector(1048576, {148618: 1.7047, 183975: 1.7047, 216207: 1.7047, 261052: 1.0116, 617454: 1.7047, 696349: 1.7047, 721336: 1.2993, 816618: 0.7885, 897662: 1.7047}),\n",
       " SparseVector(1048576, {60386: 1.2993, 177421: 1.7047, 568609: 1.7047, 569458: 1.7047, 847171: 1.7047, 850510: 1.7047, 1040679: 1.7047}),\n",
       " SparseVector(1048576, {261052: 4.0464, 816618: 3.1538}),\n",
       " SparseVector(1048576, {60386: 5.1971, 594754: 6.819}),\n",
       " SparseVector(1048576, {21980: 1.7047, 70882: 1.7047, 274690: 1.7047, 357784: 1.7047, 549790: 1.7047, 597434: 1.7047, 804583: 1.7047, 829803: 1.7047, 935701: 1.2993}),\n",
       " SparseVector(1048576, {154253: 1.7047, 261052: 1.0116, 438276: 1.7047, 460085: 1.7047, 585459: 1.7047, 664288: 1.7047, 816618: 0.7885, 935701: 2.5986, 948143: 1.7047, 1017889: 1.7047}),\n",
       " SparseVector(1048576, {270017: 1.7047, 472985: 1.7047, 511771: 1.7047, 718483: 1.7047, 820917: 1.7047}),\n",
       " SparseVector(1048576, {34116: 1.7047, 87407: 1.7047, 276491: 1.7047, 348943: 1.7047, 482882: 1.7047, 549350: 1.7047, 721336: 1.2993, 816618: 0.7885, 1025622: 1.7047}),\n",
       " SparseVector(1048576, {1769: 1.7047, 151357: 1.7047, 500659: 1.7047, 547760: 1.7047, 979482: 1.7047})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. StandardScaler\n",
    "\n",
    "데이터를 표준화하려면 1) 평균과 표준편차를 계산하고, 2) 측정값에서 평균을 빼고, 표준편차로 나누어 주면 된다.\n",
    "즉 zscore를 계산하는 것과 같다.\n",
    "\n",
    "$$ z = \\frac {\\bar{x_n} - \\mu} {\\sigma / \\sqrt{n}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRdd = spark.sparkContext\\\n",
    "    .textFile(os.path.join('s-master/data', 'ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '65.78', '112.99']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규화 할 값만 추출\n",
    "## 탭을 분리한다\n",
    "tRdd.map(lambda x: x.split('\\t')).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 65.78, 112.99]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형 변환을 해준다\n",
    "tRdd.map(lambda x: x.split('\\t')).map(lambda x: [str(x[0]), float(x[1]), float(x[2])]).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 65.78, 112.99]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tRdd.map(lambda x: x.split('\\t'))\\\n",
    "    .map(lambda x: [str(x[0]), float(x[1]), float(x[2])])\\\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2개의 값만 추출하여 dense vectors에 별도로 저장한다.\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "_tRdd =tRdd\\\n",
    "    .map(lambda x: x.split('\\t'))\\\n",
    "    .map(lambda x: [str(x[0]), float(x[1]), float(x[2])])\\\n",
    "    .map(lambda x: Vectors.dense([x[1], x[2]])) #두개만 가져와서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리스트로 저장해도 계산에 문제가 없다.\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "_tRdd =tRdd\\\n",
    "    .map(lambda x: x.split('\\t'))\\\n",
    "    .map(lambda x: [str(x[0]), float(x[1]), float(x[2])])\\\n",
    "    .map(lambda x: [x[1], x[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 표준화\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "scaler1 = StandardScaler().fit(_tRdd) #평균,표준편차 넣어서 해도되고 없이 해도되고\n",
    "scaler2 = StandardScaler(withMean=True, withStd=True).fit(_tRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-1.2458, -1.2299]),\n",
       " DenseVector([1.9011, 0.5934]),\n",
       " DenseVector([0.7388, 1.8767]),\n",
       " DenseVector([0.0919, 1.0473]),\n",
       " DenseVector([-0.1439, 1.1993])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zscore와 비교\n",
    "##fit 다음에는 transform 하게 된다.\n",
    "### StandardScaler해주면 zscore 값을 계산없이 구할 수 있다.\n",
    "scaler2.transform(_tRdd).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame으로 만들어진 데이터를 변환해보자. <br>\n",
    "이러한 작업이 필요한 이유는 **기계학습에 넘겨줄 입력데이터를 형식에 맞추어야** 하기 때문이다.<br>\n",
    "데이터는 형식에 맞게 변환되고, 군집화, 회귀분석, 분류, 추천 모델 등에 입력으로 사용된다<br>\n",
    "물론 데이터는 '일련의 수' 또는 '텍스트'로 구성된다.<br>\n",
    "이런 데이터로부터 특징을 추출하여 **feature vectors**를 구성한다.<br>\n",
    "지도학습을 하는 경우에는 **class 또는 label** 값이 필요하다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.1 Labeled Point를 label, features 컬럼으로 분해\n",
    "\n",
    "RDD LabeledPoint는 label과 vectors로 구성되어 있다.\n",
    "따라서 LabeledPoint를 DataFrame으로 읽어오면, 2개의 컬럼으로 생성된다.\n",
    "이를 label, features 컬럼으로 맞추어 주도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 레이블이 있는 Python List에서 DataFrame 생성\n",
    "\n",
    "label과 features를 가지고 가지고 있는 데이터를 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [[1, [1.0, 2.0, 3.0]], [1, [1.1, 2.1, 3.1]], [0, [1.2, 2.2, 3.3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n",
      "features: [1.0, 2.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "print (\"label: {}\\nfeatures: {}\".format(p[0][0], p[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 데이터를 읽어서 DataFrame을 생성하면, 두 개의 컬럼으로 구분해서 생성된다. <br>\n",
    "그러나 컬럼이 자동 명명되기 때문에 ```_1, _2```가 쓰여서 만족스럽지 못하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabeledPoint에서 DataFrame 생성\n",
    "\n",
    "Python List를 LabeledPoint로 만들어 보자.\n",
    "**LabeledPoint는 RDD에서 사용하는 구조로서 mllib 라이브러리를 사용**해서 만들고 있다.\n",
    "**DataFrame은 LabeledPoint를 컬럼으로 가지고 있지 않는다**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1, [1.0,2.0,3.0]),\n",
    "     LabeledPoint(1, [1.1,2.1,3.1]),\n",
    "     LabeledPoint(0, [1.2,2.2,3.3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe 생성\n",
    "trainDf=spark.createDataFrame(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 LabeledPoint는 분해되어, **label과 features를 별도 컬럼**으로 생성된다. 이런 명칭의 컬럼은 기계학습에 필요하다. **features 데이터를 모델링하여 label에 따라 분류**하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mllib.linalg.Vectors를 사용하여 DataFrame을 생성\n",
    "\n",
    "앞서 mllib와 ml 모듈을 섞어서 사용하지 않아야 한다고 했다.\n",
    "mllib vectors를 사용해도 DataFrame을 생성하는데 문제가 없다.\n",
    "컬럼명을 ```[\"label\", \"features\"]```으로 하지 않으면, 자동명명되니 주의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainDf = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, 0.5]))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD에서 DataFrame 생성\n",
    "\n",
    "rdd에서 DataFrame을 생성하면 label, features이 당연히 생성이 되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.mllib.linalg import SparseVector # mllib ok\n",
    "from pyspark.ml.linalg import SparseVector # ml ok\n",
    "\n",
    "_rdd = spark.sparkContext.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: double (nullable = true)\n",
      " |-- _2: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df=_rdd.toDF()\n",
    "_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0| (4,[1,3],[1.0,5.5])|\n",
      "|  1.0|(4,[0,2],[-1.0,0.5])|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df=_df.withColumnRenamed('_1', 'label').withColumnRenamed('_2', 'features')\n",
    "_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.2 단어 빈도\n",
    "\n",
    "정량 데이터는 합계, 평균, 표준편차 등 의미있는 통계량을 계산하거나, 이런 통계량이 집단 간에 차이가 있는지 분석한다.\n",
    "반면에 텍스트는 정량데이터와 같이 이러한 통계량의 계산이 가능하지 못하게 된다.\n",
    "텍스트에 어떤 단어가 얼마나 쓰였는지, 또한 같이 쓰이게 된 단어는 무엇인지 등 단어의 빈도에 따라 **정량화하여 과학적인 분석**을 하게 된다.\n",
    "\n",
    "#### Bag of Words 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 변환 단계\n",
    "\n",
    "텍스트를 변환하는 단계를 보자. 순서는 변경될 수 있다.\n",
    "\n",
    "* 단계 1: 단어로 분할 Tokenization\n",
    "    * 그, 영화는, 매우, 강렬했다, 그냥, 좋았다, 영화관에서, 보는, 동안, 긴장을, 늦출, 수, 없었다, 갑돌이가, 분장한, 악당의, 케릭터가, 만들어지는, 과정은, 흥미롭지, 않을, 수가, 없었다, 무비의, 이야기, 전개는, 빠르고, 무엇이, 진실이고, 거짓인지, 판단할, 수, 없었다, 누가, 이런, 영화를, 좋아, 하지, 않을, 수가, 있겠는가, 이모티콘\n",
    "\n",
    "* 단계 2: 정리\n",
    "    - 불필요, 오타 등\n",
    "\n",
    "* 단계 3: 불용어 stopwords 제거\n",
    "    - 그, 수, 수가, 수, 이런, 하지, 수가 등\n",
    "\n",
    "* 단계 4: 어간 추출 stemming\n",
    "    영화는, 영화의는 다른 단어지만 조사를 제거하면 동일한 단어\n",
    "    - 좋았다, 좋아 단어들은 어근을 판별하면 동일한 단어이다.\n",
    "    - 영화, 무비의 단어는 이음동의\n",
    "* 단계 5: 계량화\n",
    "    - word vector로 만든다.\n",
    "    - 있다-없다, 단어빈도, TF-IDF 사용할 수 있다.<br>dense, sparse 모두 가능하다.\n",
    "    ```[1,1,1,1,1,0,0],[0,1,0,1,1,1,1]```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let it be lyrics\n",
    "doc=[\n",
    "    \"When I find myself in times of trouble\",\n",
    "    \"Mother Mary comes to me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"And in my hour of darkness\",\n",
    "    \"She is standing right in front of me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Whisper words of wisdom, let it be\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서는 문장으로 구성되어 있고, 문장은 단어로 구성되어 있다. 따라서 첫째 반복문은 문서의 각 문장에 대해, 단어로 분리하고 있다. 그 다음 반복문은 각 단어에 대해 빈도를 계산한다. 각 단어가 키가 되는데, 키가 존재하면 빈도를 증가하고, 존재하지 않으면 새로운 키를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "for sentence in doc:\n",
    "    words=sentence.split()\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word]+=1\n",
    "        else:\n",
    "            d[word]=1\n",
    "            \n",
    "#단어의 빈도를 DICTIONARY d에 저장 키, 빈도의 쌍으로 저장되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'When': 1,\n",
       " 'I': 1,\n",
       " 'find': 1,\n",
       " 'myself': 1,\n",
       " 'in': 3,\n",
       " 'times': 1,\n",
       " 'of': 6,\n",
       " 'trouble': 1,\n",
       " 'Mother': 1,\n",
       " 'Mary': 1,\n",
       " 'comes': 1,\n",
       " 'to': 1,\n",
       " 'me': 2,\n",
       " 'Speaking': 2,\n",
       " 'words': 3,\n",
       " 'wisdom,': 3,\n",
       " 'let': 3,\n",
       " 'it': 7,\n",
       " 'be': 7,\n",
       " 'And': 1,\n",
       " 'my': 1,\n",
       " 'hour': 1,\n",
       " 'darkness': 1,\n",
       " 'She': 1,\n",
       " 'is': 1,\n",
       " 'standing': 1,\n",
       " 'right': 1,\n",
       " 'front': 1,\n",
       " 'Let': 4,\n",
       " 'Whisper': 1}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\t1\n",
      "I\t1\n",
      "find\t1\n",
      "myself\t1\n",
      "in\t3\n",
      "times\t1\n",
      "of\t6\n",
      "trouble\t1\n",
      "Mother\t1\n",
      "Mary\t1\n",
      "comes\t1\n",
      "to\t1\n",
      "me\t2\n",
      "Speaking\t2\n",
      "words\t3\n",
      "wisdom,\t3\n",
      "let\t3\n",
      "it\t7\n",
      "be\t7\n",
      "And\t1\n",
      "my\t1\n",
      "hour\t1\n",
      "darkness\t1\n",
      "She\t1\n",
      "is\t1\n",
      "standing\t1\n",
      "right\t1\n",
      "front\t1\n",
      "Let\t4\n",
      "Whisper\t1\n"
     ]
    }
   ],
   "source": [
    "# for k,v in d.iteritems():  # python2\n",
    "for k,v in d.items():\n",
    "    print (\"{}\\t{}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.3 Spark의 transformer, estimator\n",
    "\n",
    "**RDD**를 만들고 나서도 데이터를 변환하기 위해 map-reduce와 같은 함수 또는  **transform()**, **fit()**을 사용하는 것과 같이,\n",
    "**DataFrame**도 역시 **Transformer**, **Estimator**를 사용할 수 있다.\n",
    "이러한 Spark ml 라이브러리는 Python의 scikit-learn에서 영향을 받아 기계학습 API transformer, estimator, evaluator가 유사하다.\n",
    "* Estimator는 DataFrame에 적용되는 알고리즘을 말하는 것으로, Transformer를 생성해낸다.<br> **```Estimator.fit()```**\n",
    "* Transformer는 DataFrame을 적용해서 다른 DataFrame으로 생성한다. <br>**```Transformer.transform()```**\n",
    "* Evaluator는 ```pyspark.ml.evaluation```의 'BinaryClassificationEvaluator', 'RegressionEvaluator',  'MulticlassClassificationEvaluator', 'MultilabelClassificationEvaluator', 'ClusteringEvaluator', 'RankingEvaluator' 등이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2차원 배열\n",
    "doc2d=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(doc2d, ['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                sent|\n",
      "+--------------------+\n",
      "|When I find mysel...|\n",
      "|Mother Mary comes...|\n",
      "|Speaking words of...|\n",
      "|And in my hour of...|\n",
      "|She is standing r...|\n",
      "|Speaking words of...|\n",
      "|      우리 Let it be|\n",
      "|        나 Let it be|\n",
      "|        너 Let it be|\n",
      "|           Let it be|\n",
      "|Whisper words of ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#truncate 긴 단어들 잘라버림\n",
    "myDf.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.4 Tokenizer\n",
    "\n",
    "먼저 용어를 정리해 보자.\n",
    "* corpus는 어떤 주제에 대해 쓰여지거나, 어떤 사람이 작성한 전체 '말뭉치'를 말한다. 여러 문장으로 구성된 텍스트 집합을 말한다.\n",
    "* document는 문장으로 구성된 문서를 말하지만, 한 문장으로만 구성될 수도, 여러 문장으로 만들어질 수도 있다. 예를 들어, \"why she had to go\" 같은 한 문장도 document라고 하고, \"why she had to go.. I don't know\" 역시 마찬가지이다.\n",
    "* vocabularay는 중복없는 단어 집합을 말하며, 예를 들면, \"why\",\"she\",\"had\",\"to\",\"go\",\"where\",\"have\" 등은 단어이다.\n",
    "\n",
    "Tokenizer는 document를 단어로 분리한다.\n",
    "분리하는 기준은 whitespace로 공백, TAB, CR, New Line 등이 해당된다.\n",
    "* 입력 컬럼은 \"sent\"로,\n",
    "* 출력 컬럼은 \"words\"로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```transform()```은 앞서 만든 ```tokenizer```모델에 DataFrame을 변환하여 다른 DataFrame을 생성한다.\n",
    "그 결과는 문자열 배열로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|               words|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf = tokenizer.transform(myDf)\n",
    "tokDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```for```문으로 출력해보자. ```Row()``` 객체로 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(sent='When I find myself in times of trouble', words=['when', 'i', 'find', 'myself', 'in', 'times', 'of', 'trouble'])\n",
      "Row(sent='Mother Mary comes to me', words=['mother', 'mary', 'comes', 'to', 'me'])\n",
      "Row(sent='Speaking words of wisdom, let it be', words=['speaking', 'words', 'of', 'wisdom,', 'let', 'it', 'be'])\n"
     ]
    }
   ],
   "source": [
    "for r in tokDf.select(\"sent\", \"words\").take(3):\n",
    "    print (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.5 RegTokenizer\n",
    "\n",
    "Tokenizer는 white space로 분리하지만, RegexTokenizer는 단어를 분리하기 위해 **정규표현식**을 적용할 수 있다.\n",
    "정규표현식을 사용하여 분리하거나 특정 패턴을 추출할 수 있다.\n",
    "공백으로 분리할 경우 간단히 정규표현식 ```\\s``` 패턴을 적용할 수 있다.\n",
    "한글에는 ```\\w``` 패턴이 적용되지 않는다.\n",
    "* ```\\s```는 공백문자\n",
    "* ```\\w```는 숫자 및 대소문자 ```[A-Za-z0-9_]```\n",
    "* 별표 ```*```는 0 또는 그 이상, 더하기 ```+```는 1 또는 그 이상을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|            wordsReg|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|\n",
      "|        나 Let it be|   [나, let, it, be]|\n",
      "|        너 Let it be|   [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reDf=re.transform(myDf)\n",
    "reDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|               words|            wordsReg|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]| [우리, let, it, be]|\n",
      "|        나 Let it be|   [나, let, it, be]|   [나, let, it, be]|\n",
      "|        너 Let it be|   [너, let, it, be]|   [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reDf1=re.transform(tokDf)\n",
    "reDf1.show()\n",
    "## 정교하게 내가 원하는 데이터를 가지고 할 수 있다는 장점이 있음 : 그러므로 정규 토크나이저를 더 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.6 Stopwords\n",
    "\n",
    "텍스트를 분리하고 나면, 별 의미가 없거나 쓸모가 없는 단어들이 존재한다.\n",
    "예를 들어 이, 그, 저와 같은 **한 단어** 또는 있다 등과 같은 **일부 동사**, 그래서, 그러나 등과 같은 **접속사** 등이 후보가 될 수 있다.\n",
    "이런 불필요한 단어들을 불용어 Stopwords라고 하며, 입력데이터에서 제거하도록 한다.\n",
    "영어의 경우 불용어가 식별되어 제공되고 있다\n",
    "http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"wordsReg\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=list()\n",
    "#stopwords 에 현재 불용어를 담는다\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_01cde1971dd1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"] #유니코드로 인식하여서 가지고옴\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i/me/my/myself/we/our/ours/ourselves/you/your/yours/yourself/yourselves/he/him/his/himself/she/her/hers/herself/it/its/itself/they/them/their/theirs/themselves/what/which/who/whom/this/that/these/those/am/is/are/was/were/be/been/being/have/has/had/having/do/does/did/doing/a/an/the/and/but/if/or/because/as/until/while/of/at/by/for/with/about/against/between/into/through/during/before/after/above/below/to/from/up/down/in/out/on/off/over/under/again/further/then/once/here/there/when/where/why/how/all/any/both/each/few/more/most/other/some/such/no/nor/not/only/own/same/so/than/too/very/s/t/can/will/just/don/should/now/i'll/you'll/he'll/she'll/we'll/they'll/i'd/you'd/he'd/she'd/we'd/they'd/i'm/you're/he's/she's/it's/we're/they're/i've/we've/you've/they've/isn't/aren't/wasn't/weren't/haven't/hasn't/hadn't/don't/doesn't/didn't/won't/wouldn't/shan't/shouldn't/mustn't/can't/couldn't/cannot/could/here's/how's/let's/ought/that's/there's/what's/when's/where's/who's/why's/would/나/너/우리/나/너/우리/"
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print (e, end=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|               [let]|\n",
      "|        나 Let it be|   [나, let, it, be]|               [let]|\n",
      "|        너 Let it be|   [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.7 CountVectorizer\n",
    "\n",
    "```CountVectorizer```는 단어의 빈도 수를 계산한다.\n",
    "\n",
    "3번째 문장 \"Speaking words of wisdom, let it be\"의 word vector를 구성해 본다.\n",
    "id 값은 모든 문장에서 단어를 추출하고 나서야 부여된다.\n",
    "\n",
    "단어 (3행 \"Speaking words of wisdom, let it be\") | id | 빈도 | \n",
    "-----|-----|-----\n",
    "Speaking | 7 | 1\n",
    "words | 13 | 1\n",
    "of | stopword | 0\n",
    "wisdom | 12 | 1\n",
    "let | 3 | 1\n",
    "it | stopword | 0\n",
    "be | stopword | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 **word vector**를 표로 나타내면 아래와 같다.\n",
    "행은 문장, 열은 id이다.\n",
    "**3행은 doc2**이다. 해당하는 **단어 id의 빈도**를 적었다. 다른 행과 열은 이해를 돕기 위해 비워 놓았다.\n",
    "\n",
    "```doc``` \\ 단어 id  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |10 |11 |12 |13 |...\n",
    "------|---|---|---|---|---|---|---|---|---|---|---|---|---|---\n",
    "```doc 0``` |   |   |   |   |   |   |   |   |   |   |   |   |   |...\n",
    "```doc 1``` |   |   |   |   |   |   |   |   |   |   |   |   |   |...\n",
    "```doc 2``` |   |   | 1 |   |   |   | 1 |   |   |   |   | 1 | 1 |...\n",
    "...   |   |   |   |   |   |   |   |   |   |   |   |   |   |..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn CountVectorizer\n",
    "\n",
    "sklearn 라이브러리로 단어빈도를 세어보자.\n",
    "입력으로 단어의 집합을 넣어야 하지만, 위 문서는 2차원 리스트이다.\n",
    "먼저 2차원 리스트를 1차원으로 변경하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce #2차원을 1차원으로 만들어줘야함\n",
    "doc = reduce(lambda x,y: x+y, doc2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 6)\t1\n",
      "  (5, 7)\t1\n",
      "  (5, 13)\t1\n",
      "  (5, 12)\t1\n",
      "  (5, 3)\t1\n",
      "  (6, 3)\t1\n",
      "  (6, 14)\t1\n",
      "  (7, 3)\t1\n",
      "  (8, 3)\t1\n",
      "  (9, 3)\t1\n",
      "  (10, 13)\t1\n",
      "  (10, 12)\t1\n",
      "  (10, 3)\t1\n",
      "  (10, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'times': 9,\n",
       " 'trouble': 10,\n",
       " 'mother': 5,\n",
       " 'mary': 4,\n",
       " 'comes': 0,\n",
       " 'speaking': 7,\n",
       " 'words': 13,\n",
       " 'wisdom': 12,\n",
       " 'let': 3,\n",
       " 'hour': 2,\n",
       " 'darkness': 1,\n",
       " 'standing': 8,\n",
       " 'right': 6,\n",
       " '우리': 14,\n",
       " 'whisper': 11}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(doc).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark CountVectorizer\n",
    "\n",
    "CountVectorizer는 단어를 분리하고 나서, 빈도를 계산할 수 있다. 결과는 단어빈도 word vector (sparse), 즉 단어별 단어빈도 TF이다.\n",
    "자주 사용된 단어가 아니어서 제거해야할 단어, 즉 문서에 사용된 빈도 document frequency를 minDF, maxDF를 통해 설정할 수 있다.\n",
    "소수점을 사용하면, 비율, 즉 '사용된 문서 수/전체 문서 수'를 의미한다.\n",
    "\n",
    "* minDf는 너무 **적게 발생하는 경우 무시**, 예를 들어 0.5는 전체 문서의 **50%보다 적게** 발생하는 단어는 무시, 1.0은 기본 값이고, **100%보다 적게** 발생하는 경우 무시하게 된다. 즉, minDf=1.0은 문서 1개 이하에서 나타난 단어는 무시하라는 의미이다. 즉 어떤 단어도 무시하지 말라는 의미이다.\n",
    "* maxDf는 너무 **많이 발생하는 경우 무시**, 예를 들어 0.5는 전체 문서의 **50%보다 많이** 발생하는 경우 무시, 1.0은 **100%보다 많이** 발생하는 경우 무시 (즉, 어떤 단어도 무시하지 말라는 의미). min_df와 마찬가지로 1.0이 기본 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\", vocabSize=30, minDF=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```CountVectorizerModel```은 ```fit()```하고 나면 얻어진다. 다음에 사용하는 ```HashingTF```는 ```fit()```하지 않는다는 점에서 차이가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.feature.CountVectorizer'> <class 'pyspark.ml.feature.CountVectorizerModel'>\n"
     ]
    }
   ],
   "source": [
    "print (type(cv),type(cvModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf = cvModel.transform(stopDf)\n",
    "cvDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame 전체를 출력하면 보기 불편하므로, 이 가운데 일부 컬럼만을 선택하여 출력할 수 있다.\n",
    "```(16,[5,6,8],[1.0,1.0,1.0])```\n",
    "16은 전체 단어의 개수, 그리고 다음 5,6,8은 값이 있는 컬럼 번호, 1.0,1.0,1.0은 그 값을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|And in my hour of...|    [hour, darkness]|(16,[7,9],[1.0,1.0])|\n",
      "|She is standing r...|[standing, right,...|(16,[4,12,15],[1....|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|      우리 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|        나 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|        너 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|           Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|Whisper words of ...|[whisper, words, ...|(16,[0,1,2,11],[1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.select('sent','nostops','cv').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```CountVectorizer```에서 사용된 단어 목록을 출력할 수 있다. 아래 단어의 수를 세어보면 위 sparse vector의 컬럼 개수와 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let',\n",
       " 'wisdom,',\n",
       " 'words',\n",
       " 'speaking',\n",
       " 'right',\n",
       " 'trouble',\n",
       " 'find',\n",
       " 'hour',\n",
       " 'times',\n",
       " 'darkness',\n",
       " 'mother',\n",
       " 'whisper',\n",
       " 'front',\n",
       " 'mary',\n",
       " 'comes',\n",
       " 'standing']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.8 TF-IDF\n",
    "\n",
    "```TfidfTransformer```는 **TF-IDF(Term Frequency-Inverse Document Frequency)**를 계산한다.\n",
    "이를 위해서는 우선 Tokenizer를 사용하여 문장을 단어로 분리해 놓아야 한다.\n",
    "\n",
    "HashingTF를 사용하여 'word vector'를 계산한다.\n",
    "HashingTF은 hash함수에 따라 단어의 고유번호를 생성하며, 단어 수가 많아지면서 고유번호가 충돌할 수 있는 가능성이 있지만 이를 최소화게 된다.\n",
    "그리고 IDF를 계산하고, TF-IDF를 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S.5.8.1 TF-IDF 계산\n",
    "\n",
    "'Let it be'가사 세 번째 줄 **'wisdom' 단어**의 TF-IDF를 계산해보자.\n",
    "**TF**는 단어빈도수, 즉 문서에 단어가 나타난 빈도수를 의미한다.\n",
    "단어빈도는 경우에 따라서는 문제가 될 수 있다. 예를 들어, 'a', 'the', 'of'와 같은 단어는 빈도는 높지만 별로 유용하지 못하다.\n",
    "이 경우 IDF는 유용하다. **IDF**는 자주 나타나는 단어에 대한 가중치를 줄이고, 드물게 나타나는 단어에 가중치를 높이는 방식으로 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t는 단어, 문서는 d, D는 corpus,\n",
    "\n",
    "항목 | 설명 | 예제\n",
    "-----|-----|-----\n",
    "tf(d,f) | 단어 t가 문서 d에서 나타나는 단어의 빈도 수, term frequency | $f_{t,d}$ / (number of words in d) = 1/4 = 0.25<br>(3번째 문서에 stopwords를 제외하면 4개의 단어, wisdom은 1회 나타난다.)\n",
    "df | document frequency 단어가 나타난 문서 수 | 3 (wisdom이 포함된 문서는 3)\n",
    "N | number of documents 전체 문서의 수 | 11 (전체의 문서는 11개)\n",
    "idf | inverse document frequency 단어가 나타난 문서의 비율을 거꾸로 | ln(N+1 / df+1) + 1 = log(12/4) + 1 = 1.09861 + 1<br>0으로 나뉘는 것을 방지하기 위해 **smoothing**, 즉 1을 더한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로그래밍에서는 메모리를 적게 사용하도록 설계되어 있다. ```1/4```와 같이 정수 타잎으로 연산하면, 정수를 사용하여 연산하고 그 결과도 정수를 출력하게 된다. 이를 변환하기 위해 ```1.```의 경우에서와 같이 소수를 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf: 2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "tf=1./4\n",
    "df=3.\n",
    "N=11.\n",
    "idf=math.log((N+1)/(df+1))+1\n",
    "print (\"idf: {}\".format(idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S.5.8.2 sklearn을 사용한 TF-IDF\n",
    "\n",
    "우선 'sklearn'의 TF-IDF를 계산해보자.\n",
    "**```CountVectorizer```**는 텍스트를 단어의 빈도로 변환해주어, 문서 x 단어 표를 출력할 수 있다.\n",
    "**```CountVectorizer()```**의 인자로\n",
    "analyzer (\"word\", \"character ngram\" 등 선택),\n",
    "tokenizer (단어의 tokenizer를 지정),\n",
    "stop_words (불용어 처리 기준),\n",
    "max_features (최대 속성 개수) 등을 지정할 수 있다.\n",
    "그 다음으로, TF-IDF를 계산할 수 있다. 이 때 (문서id, 단어id) 별로 결과가 출력된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```TfidfVectorizer```를 사용해서 계산하면 그 결과를 아래와 같이 볼 수 있다.\n",
    "```python\n",
    "(2,12) 2.09861228867\n",
    "```\n",
    "\n",
    "결과에서\n",
    "**'2'**는 3번째 문서번호, **'12'**는 'wisdom' 단어번호\n",
    "TF-IDF는 ```2.09861228867```이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_df, min_df는 기본 값이 1.0으로 굳이 설정하지 않아도 되는데, 어떤 단어도 무시하지 말라는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t2.791759469228055\n",
      "  (0, 9)\t2.791759469228055\n",
      "  (1, 0)\t2.791759469228055\n",
      "  (1, 4)\t2.791759469228055\n",
      "  (1, 5)\t2.791759469228055\n",
      "  (2, 3)\t1.4054651081081644\n",
      "  (2, 12)\t2.09861228866811\n",
      "  (2, 13)\t2.09861228866811\n",
      "  (2, 7)\t2.386294361119891\n",
      "  (3, 1)\t2.791759469228055\n",
      "  (3, 2)\t2.791759469228055\n",
      "  (4, 6)\t2.791759469228055\n",
      "  (4, 8)\t2.791759469228055\n",
      "  (5, 3)\t1.4054651081081644\n",
      "  (5, 12)\t2.09861228866811\n",
      "  (5, 13)\t2.09861228866811\n",
      "  (5, 7)\t2.386294361119891\n",
      "  (6, 14)\t2.791759469228055\n",
      "  (6, 3)\t1.4054651081081644\n",
      "  (7, 3)\t1.4054651081081644\n",
      "  (8, 3)\t1.4054651081081644\n",
      "  (9, 3)\t1.4054651081081644\n",
      "  (10, 11)\t2.791759469228055\n",
      "  (10, 3)\t1.4054651081081644\n",
      "  (10, 12)\t2.09861228866811\n",
      "  (10, 13)\t2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, stop_words='english',norm = None)\n",
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'times': 9,\n",
       " 'trouble': 10,\n",
       " 'mother': 5,\n",
       " 'mary': 4,\n",
       " 'comes': 0,\n",
       " 'speaking': 7,\n",
       " 'words': 13,\n",
       " 'wisdom': 12,\n",
       " 'let': 3,\n",
       " 'hour': 2,\n",
       " 'darkness': 1,\n",
       " 'standing': 8,\n",
       " 'right': 6,\n",
       " '우리': 14,\n",
       " 'whisper': 11}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.79175947, 2.79175947, 2.79175947, 1.40546511, 2.79175947,\n",
       "       2.79175947, 2.79175947, 2.38629436, 2.79175947, 2.79175947,\n",
       "       2.79175947, 2.79175947, 2.09861229, 2.09861229, 2.79175947])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S.5.8.3 Spark를 사용한 TF-IDF\n",
    "\n",
    "##### TF\n",
    "HashingTF는 단어집합을 워드벡터 word vector로 변환하는데, 해시함수를 사용해서 단어에 해당하는 일련번호를 결정한다.\n",
    "HashingTF에서의 **```numFeatures```는 $2^n$**으로 결정하게 된다.\n",
    "단어 갯수가 900이면, $2^{10}=1024$이므로 1024로 설정하면 된다.\n",
    "기본은 $2^{18}=262,144$이다.\n",
    "너무 적게 설정되면 인덱스가 부족하거나 적절하게 매핑될 수 있으니 주의해야 한다.\n",
    "\n",
    "\n",
    "예를 들어, \n",
    "문서 ```[speaking, words, wisdom,, let]```의 경우 ```(32,[4,24,27],[1.0,1.0,2.0])```가 출력된다.\n",
    "아래의 결과와 비교하면 단어 하나가 유실된 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=32)#numfeatrures의 기본 값은 262,144 너무 크므로 줄일라면\n",
    "#  mapping indices insufficient\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```HashingTF```는 ```fit()```하지 않고 ```transform()``` 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashDf = hashTF.transform(stopDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(50,[10,24,43],[1.0,1.0,1.0])```\n",
    "16은 해시 개수 (앞서 CountVectorizer의 경우에서와 같이 전체 단어의 개수가 아니다), 그리고 다음 [10,24,43]은 값이 있는 **해시 컬럼** 번호, 1.0,1.0,1.0은 그 값을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+--------------------------------------------------------+\n",
      "|nostops                        |hash                                                    |\n",
      "+-------------------------------+--------------------------------------------------------+\n",
      "|[find, times, trouble]         |(262144,[64317,91878,152481],[1.0,1.0,1.0])             |\n",
      "|[mother, mary, comes]          |(262144,[24657,63767,245426],[1.0,1.0,1.0])             |\n",
      "|[speaking, words, wisdom,, let]|(262144,[27556,151864,173339,175131],[1.0,1.0,1.0,1.0]) |\n",
      "|[hour, darkness]               |(262144,[74517,98431],[1.0,1.0])                        |\n",
      "|[standing, right, front]       |(262144,[84798,218360,229166],[1.0,1.0,1.0])            |\n",
      "|[speaking, words, wisdom,, let]|(262144,[27556,151864,173339,175131],[1.0,1.0,1.0,1.0]) |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[whisper, words, wisdom,, let] |(262144,[151864,173339,175131,188139],[1.0,1.0,1.0,1.0])|\n",
      "+-------------------------------+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashDf.select(\"nostops\", \"hash\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nostops=['find', 'times', 'trouble'], hash=SparseVector(262144, {64317: 1.0, 91878: 1.0, 152481: 1.0}))\n",
      "Row(nostops=['mother', 'mary', 'comes'], hash=SparseVector(262144, {24657: 1.0, 63767: 1.0, 245426: 1.0}))\n",
      "Row(nostops=['speaking', 'words', 'wisdom,', 'let'], hash=SparseVector(262144, {27556: 1.0, 151864: 1.0, 173339: 1.0, 175131: 1.0}))\n",
      "Row(nostops=['hour', 'darkness'], hash=SparseVector(262144, {74517: 1.0, 98431: 1.0}))\n",
      "Row(nostops=['standing', 'right', 'front'], hash=SparseVector(262144, {84798: 1.0, 218360: 1.0, 229166: 1.0}))\n",
      "Row(nostops=['speaking', 'words', 'wisdom,', 'let'], hash=SparseVector(262144, {27556: 1.0, 151864: 1.0, 173339: 1.0, 175131: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)\n",
    "for e in idfDf.select(\"nostops\",\"hash\").take(10):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "- Bags of Words 모델은 단어 순서와 문맥을 무시한다.\n",
    "- Word2Vec은 이런 BoW 모델의 단점을 극복하기 위해서 말뭉치로부터 **단어들 서로의 맥락 또는 연관성 Word Embedding**을 신경망으로 학습하여 Word2Vec을 계산\n",
    "- 단어를 벡터로 변환하게 되면 벡터 연산이 가능해지고 서로 간의 거리를 측정하여, 가까울 수록 비슷한 단어를 해석 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "#최소 갯수 0\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"w2v\")\n",
    "# token으로 분리하여\n",
    "model = word2Vec.fit(tokDf)\n",
    "w2vDf = model.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in w2vDf.select(\"w2v\").take(3):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.getVectors().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"times\"와 유사한 단어 3개 찾아서 show\n",
    "model.findSynonyms(\"times\", 3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  NGram\n",
    "\n",
    "텍스트를 대상으로 하면, n-gram은 연속된 n개의 토큰으로 구성된 순열을 말한다.\n",
    "unigram은 한 단어로, bigram은 두 단어로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(e)? (<ipython-input-1-c47b3abea57e>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-c47b3abea57e>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    print e\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(e)?\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "#tokenizer 되어있는걸 input으로 넣음\n",
    "ngramDf = ngram.transform(tokDf) \n",
    "ngramDf.show()\n",
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print e\n",
    "    \n",
    "# bigram으로 넣는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringIndexer\n",
    "\n",
    "문자열 컬럼을 인덱스 컬럼으로 변환한다. **빈도가 제일 높은 순서**로 ```0.0```부터 인덱스 값이 주어진다. 인덱스는 double 형을 가지게 된다.\n",
    "없는 레이블에 대해서는 예외가 발생할 수 있으므로 (default), ```setHandleInvalid(\"skip\")``` 함수로 'skip', 'keep', 'error' 등 처리\n",
    "\n",
    "구분 | 설명 | 예\n",
    "-----|-----|-----\n",
    "nominal | 명목 또는 구분 값 cateogry  | 사자, 호랑이, 사람\n",
    "ordinal | 명목값과 다른 점은 순서가 있다. | 키 low, med, high\n",
    "interval | 일정한 간격이 있다. | 150-165, 165-180, 180-195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 텍스트에 대해서는 적당한 명목 변수가 없으므로, 문장 전체에 대해 인덱스로 반환\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")\n",
    "model=labelIndexer.fit(myDf)\n",
    "siDf=model.transform(myDf)\n",
    "siDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연속데이터의 변환\n",
    "\n",
    "몸무게(inches), 키(pounds) 데이터를 분석해보자.\n",
    "이 데이터는 정량, 연속 데이터이다. \n",
    "출처는 https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv\n",
    "\n",
    "```python\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "myRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\")\n",
    "binDf = binarizer.transform(myDf)\n",
    "binDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")\n",
    "qdDf = discretizer.fit(binDf).transform(binDf)\n",
    "qdDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler\n",
    "\n",
    "* 열을 묶어서 Vector열로 만든다.\n",
    "* string은 묶을 수 없다.\n",
    "* pyspark.ml.linalg.Vectors를 사용한다. (주의: pyspark.mllib.linalg.Vectors를 사용하지 않는다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"weight2\",\"height3\"],outputCol=\"features\")\n",
    "vaDf = va.transform(qdDf)\n",
    "vaDf.printSchema()\n",
    "vaDf.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
